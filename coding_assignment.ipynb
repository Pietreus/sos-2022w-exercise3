{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cf75c3",
   "metadata": {},
   "source": [
    "# SOM Coding\n",
    "## Group Topic B Dendrogramm\n",
    "- Peter Blohm (11905150)\n",
    "- Martin Braunsperger (11909911)\n",
    "- Adrian Gawor (11905152)\n",
    "\n",
    "## Github\n",
    "https://github.com/Pietreus/sos-2022w-exercise3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d732a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import tree\n",
    "import pandas as pdcoding\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOMToolbox Parser\n",
    "from SOMToolBox_Parse import SOMToolBox_Parse\n",
    "idata = SOMToolBox_Parse(\"datasets/10clusters/10clusters.vec\").read_weight_file()\n",
    "weights = SOMToolBox_Parse(\"datasets/iris/iris.wgt.gz\").read_weight_file()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HitHistogram\n",
    "def HitHist(_m, _n, _weights, _idata):\n",
    "    hist = np.zeros(_m * _n)\n",
    "    for vector in _idata: \n",
    "        position =np.argmin(np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1)))\n",
    "        hist[position] += 1\n",
    "\n",
    "    return hist.reshape(_m, _n)\n",
    "\n",
    "#U-Matrix - implementation\n",
    "def UMatrix(_m, _n, _weights, _dim):\n",
    "    U = _weights.reshape(_m, _n, _dim)\n",
    "    U = np.insert(U, np.arange(1, _n), values=0, axis=1)\n",
    "    U = np.insert(U, np.arange(1, _m), values=0, axis=0)\n",
    "    #calculate interpolation\n",
    "    for i in range(U.shape[0]): \n",
    "        if i%2==0:\n",
    "            for j in range(1,U.shape[1],2):\n",
    "                U[i,j][0] = np.linalg.norm(U[i,j-1] - U[i,j+1], axis=-1)\n",
    "        else:\n",
    "            for j in range(U.shape[1]):\n",
    "                if j%2==0: \n",
    "                    U[i,j][0] = np.linalg.norm(U[i-1,j] - U[i+1,j], axis=-1)\n",
    "                else:      \n",
    "                    U[i,j][0] = (np.linalg.norm(U[i-1,j-1] - U[i+1,j+1], axis=-1) + np.linalg.norm(U[i+1,j-1] - U[i-1,j+1], axis=-1))/(2*np.sqrt(2))\n",
    "\n",
    "    U = np.sum(U, axis=2) #move from Vector to Scalar\n",
    "\n",
    "    for i in range(0, U.shape[0], 2): #count new values\n",
    "        for j in range(0, U.shape[1], 2):\n",
    "            region = []\n",
    "            if j>0: region.append(U[i][j-1]) #check left border\n",
    "            if i>0: region.append(U[i-1][j]) #check bottom\n",
    "            if j<U.shape[1]-1: region.append(U[i][j+1]) #check right border\n",
    "            if i<U.shape[0]-1: region.append(U[i+1][j]) #check upper border\n",
    "\n",
    "            U[i,j] = np.median(region)\n",
    "\n",
    "    return U\n",
    "\n",
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor, approach):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "\n",
    "    return sdh_m.reshape(_m, _n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5d6af",
   "metadata": {},
   "source": [
    "In the following section we define the function used for clustering the weights of the SOM and generating the lines of the dendrogram and minimum spanning tree. First we perform agglomerative clustering by using sklearn's AgglomerativeClustering with the user defined linkage method (defaults to ward). Then we call either mst_line_overlay() to generate the lines of the minimum spanning tree or clustering_line_overlay() to generate the lines of the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c977d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def Clustering(m, n, weights, n_clusters=2, linkage='ward', line_distance_cutoff = 10, max_width = 0.8, inter_cluster = False, intra_cluster = True, line_weighting_method = None):\n",
    "    \"\"\"Perform agglomerative clustering/MST on data\n",
    "\n",
    "    arguments:\n",
    "        m:  rows of som\n",
    "        n:  columns of som\n",
    "        weights:    weight vectors of instances\n",
    "        n_clusters: amount of clusters to color\n",
    "        linkage:    linkage method by agglomerative clustering or TODO 'mst'\n",
    "\n",
    "    Returns:\n",
    "        (clust_mat,line_dict) where\n",
    "        clust_mat is an array of cluster labels of shape (m,n)\n",
    "        line_dict is a dictionary containing line segements and width in the format required by holoviews\n",
    "\n",
    "    \"\"\"\n",
    "    num_samples = len(weights)\n",
    "    if linkage == \"mst\":\n",
    "        lines = mst_line_overlay(m, n, weights, line_weighting_method, max_width)\n",
    "        clustering = AgglomerativeClustering(n_clusters=1,\n",
    "                                             compute_distances=False)\n",
    "        clustering.labels_ = np.ones(m*n)\n",
    "    else:\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "                                         compute_full_tree=True,\n",
    "                                         linkage=linkage,\n",
    "                                         compute_distances=True)\n",
    "        clustering.fit(weights)\n",
    "    # clustering.labels_ now contains the labels we want to plot\n",
    "\n",
    "        lines = clustering_line_overlay(m,\n",
    "                                        n,\n",
    "                                        clustering.children_,\n",
    "                                        clustering.distances_,\n",
    "                                        num_samples,\n",
    "                                        n_clusters,\n",
    "                                        line_distance_cutoff = line_distance_cutoff,\n",
    "                                        max_width = max_width,\n",
    "                                        inter_cluster_lines = inter_cluster,\n",
    "                                        intra_cluster_lines = intra_cluster,\n",
    "                                        line_weighting_method= line_weighting_method\n",
    "        )\n",
    "\n",
    "    return clustering.labels_.reshape(m, n), {'x0':lines.T[0],'y0':lines.T[1],'x1':lines.T[2],'y1':lines.T[3], 'distance': lines.T[4]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafd762",
   "metadata": {},
   "source": [
    "The following code generates the dendrogram based on the clustering information from the agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de506ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_line_overlay(m, n, children, distances, num_samples, n_clusters, inter_cluster_lines, intra_cluster_lines, line_weighting_method, line_distance_cutoff, max_width):\n",
    "    \"\"\"defines the lines for the dendrogram line overlay\"\"\"\n",
    "\n",
    "    # full clustering information is contained in clustering.children_\n",
    "    # row i defines cluster num_samples+i by merging the two clusters in that row\n",
    "\n",
    "    lines = np.zeros((num_samples - 1,5))\n",
    "\n",
    "    cluster_positions = np.zeros((2*num_samples - 1,2)) # center of mass for each cluster to plot the line segments from\n",
    "    cluster_sizes = np.ones((2*num_samples - 1)) # number of units in a cluster to compute center of mass correctly\n",
    "    for i in range(num_samples):\n",
    "        cluster_positions[i] = [i % n + .5, m - .5 - i // n] #using modulo operations to get the position\n",
    "\n",
    "    biggest_cluster = num_samples - 1 - n_clusters\n",
    "    if inter_cluster_lines:\n",
    "        max_dist = np.max(distances)\n",
    "    else:\n",
    "        max_dist = np.max(distances[:biggest_cluster])\n",
    "\n",
    "\n",
    "    for index, (child1, child2) in enumerate(children):\n",
    "        i = index + num_samples #offset for the single unit clusters\n",
    "        s1 = cluster_sizes[child1]\n",
    "        s2 = cluster_sizes[child2]\n",
    "        cluster_sizes[i] = s1+s2\n",
    "        # calculate center of mass\n",
    "        cluster_positions[i] = (cluster_positions[child1] * s1 + cluster_positions[child2] * s2)/cluster_sizes[i]\n",
    "\n",
    "\n",
    "        if (index <= biggest_cluster and intra_cluster_lines) or (index > biggest_cluster and inter_cluster_lines):\n",
    "\n",
    "            width = 0\n",
    "            if line_distance_cutoff is None or distances[index]/max_dist <= line_distance_cutoff:\n",
    "                width = get_line_width(line_weighting_method,distances[index],max_dist,(s1+s2) / num_samples)\n",
    "            width *= max_width\n",
    "            lines[index] = np.hstack((cluster_positions[child1],cluster_positions[child2],width))\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def index_to_coords(m,n,i):\n",
    "    return i % n + .5, m - .5 - i // n\n",
    "\n",
    "def mst_line_overlay(m, n, weights, method, max_width):\n",
    "    G = nx.Graph()\n",
    "    for i,v1 in enumerate(weights):\n",
    "        for j,v2 in enumerate(weights[i+1:]):\n",
    "            distance = np.linalg.norm(v1-v2)\n",
    "            G.add_edge(i,j+i+1,weight=distance)\n",
    "\n",
    "    MST = list(tree.minimum_spanning_edges(G, algorithm=\"prim\", data=True))\n",
    "    max_weight = max(map(lambda edge_tuple: edge_tuple[2]['weight'],MST))\n",
    "    lines = []\n",
    "    for v1, v2, w in MST:\n",
    "        line = [index_to_coords(m,n,v1),index_to_coords(m,n,v2)]\n",
    "        line = list(sum(line,())) #flattens to list\n",
    "        line.append(get_line_width(method,w['weight'],max_weight)*max_width)\n",
    "        lines.append(line)\n",
    "        \n",
    "    return np.array(lines)\n",
    "\n",
    "def get_line_width(method, distance, max_dist, relative_cluster_size = 1):\n",
    "    if method is None:\n",
    "        width = 1\n",
    "    elif method == \"size\":\n",
    "        width = relative_cluster_size ** 0.5\n",
    "    elif method == \"distance\":\n",
    "        width = distance / max_dist\n",
    "    elif method == \"inverse_distance\":\n",
    "        width = (max_dist - distance) / max_dist\n",
    "    else:\n",
    "        width = 1\n",
    "    width = np.clip(width, 0, 1)\n",
    "    return width\n",
    "\n",
    "# actual_weights = weights[\"arr\"]\n",
    "# print(len(actual_weights)-1)\n",
    "# print(mst_line_overlay(weights['ydim'], weights['xdim'],actual_weights, len(actual_weights)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import holoviews as hv\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "# hithist = hv.Image(HitHist(weights['ydim'], weights['xdim'], weights['arr'], idata['arr'])).opts(xaxis=None, yaxis=None)\n",
    "# um = hv.Image(UMatrix(weights['ydim'], weights['xdim'], weights['arr'], 10)).opts(xaxis=None, yaxis=None)\n",
    "# sdh = hv.Image(SDH(weights['ydim'], weights['xdim'], weights['arr'], idata['arr'], 25, 0)).opts(xaxis=None, yaxis=None)\n",
    "# hv.Layout([hithist.relabel('HitHist').opts(cmap='kr'),\n",
    "#            um.relabel('U-Matrix').opts(cmap='jet'), sdh.relabel('SDH').opts(cmap='viridis')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92605a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "def clustering_vis(weights,n_clust,linkage,line_distance_cutoff,max_width, inter_cluster = True, intra_cluster = False, line_weighting_method = \"size\"):\n",
    "    clust,lines = Clustering(weights['ydim'], weights['xdim'], weights['arr'], n_clust, linkage, line_distance_cutoff = line_distance_cutoff, max_width = max_width, inter_cluster = inter_cluster, intra_cluster = intra_cluster, line_weighting_method = line_weighting_method)\n",
    "    img = hv.Image(clust, bounds = (0,0,weights['xdim'], weights['ydim'])).opts(cmap='viridis',width=800,height=800)\n",
    "    seg = hv.Segments(lines,['x0','y0','x1','y1'],['distance'],).opts( line_width = 5*hv.dim('distance'), color=\"k\")\n",
    "    return hv.Layout([img*seg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1813d99",
   "metadata": {},
   "source": [
    "## Demo Visualisations\n",
    "\n",
    "### Training the SOMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a847db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "from SOMToolBox_Parse import SOMToolBox_Parse\n",
    "\n",
    "def train_som(path, x, y, iterations = 10000):\n",
    "    idata = SOMToolBox_Parse(path).read_weight_file()\n",
    "    dim = idata['vec_dim']\n",
    "    som = MiniSom(y, x, dim)\n",
    "    som.pca_weights_init(idata['arr'])\n",
    "    som.train(idata['arr'], iterations)\n",
    "    weights = np.transpose(som.get_weights(),(1,0,2)).reshape(-1, som.get_weights().shape[2])\n",
    "    weight_file = {'xdim':y, 'ydim':x, 'vec_dim':dim, 'arr':weights}\n",
    "    return idata, weight_file\n",
    "\n",
    "_, som_10clusters_small_weights = train_som(\"datasets/10clusters/10clusters.vec\", 10, 10)\n",
    "_, som_10clusters_large_weights = train_som(\"datasets/10clusters/10clusters.vec\", 100, 60, 10000)\n",
    "_, som_chailink_small_weights = train_som(\"datasets/chainlink/chainlink.vec\", 10, 10)\n",
    "_, som_chainlink_large_weights = train_som(\"datasets/chainlink/chainlink.vec\", 100, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4640f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_large_weights,1000,\"single\",None,1, inter_cluster=False, intra_cluster=True)\n",
    "#som_chainlink_large_weights['arr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6671c",
   "metadata": {},
   "source": [
    "### Comparison with Java SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb71b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_10clusters_small_weights = SOMToolBox_Parse(\"datasets/java_som/10clusters_small/10clusters_small.wgt.gz\").read_weight_file()\n",
    "som_10clusters_large_weights = SOMToolBox_Parse(\"datasets/java_som/10clusters/10clusters.wgt.gz\").read_weight_file()\n",
    "som_chainlink_small_weights = SOMToolBox_Parse(\"datasets/java_som/chainlink_small/10chainlink_small.wgt.gz\").read_weight_file()\n",
    "som_chainlink_large_weights = SOMToolBox_Parse(\"datasets/java_som/chainlink/10chainlink.wgt.gz\").read_weight_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d8693",
   "metadata": {},
   "source": [
    "### Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_large_weights, 10, \"ward\", None, 0.8, intra_cluster = True, inter_cluster = False, line_weighting_method=\"inverse_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_small_weights, 10, \"mst\", None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_10clusters_large_weights, 10, \"ward\", None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_small_weights, 7, \"single\", None, 1, inter_cluster = False, intra_cluster = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_small_weights, 7, \"single\", None, 1, inter_cluster = True, intra_cluster = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_large_weights, 2500, \"single\", 0, 1)\n",
    "#more clusters than actual samples (1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f57b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_large_weights, 25, \"ward\", 2, 2)\n",
    "#chainlink structure hard to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_vis(som_chainlink_large_weights, 1, \"mst\", 1, 0.8)\n",
    "#chainlink structure hard to see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a0277",
   "metadata": {},
   "source": [
    "### Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16387897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOS3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7d072b05de1c357e4f65510c0f07fc4077000daac60b1b3ec1ca6c5c7ea3460"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
